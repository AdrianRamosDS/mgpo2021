{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicaciones de Redes Bayesianas\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/Bayes%27_Theorem_MMB_01.jpg\" width=\"400px\" height=\"300px\" />\n",
    "\n",
    "> Con el marco teórico que hemos desarrollado hasta el día de hoy, podemos revisar algunas aplicaciones interesantes de lo que hemos venido estudiando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes\n",
    "\n",
    "En la tarea de redes Bayesianas vimos el modelo de Naive Bayes:\n",
    "\n",
    "![nb](figures/NaiveBayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos que, para todo par de nodos de características $X_i$ y $X_j$, este modelo induce que:\n",
    "\n",
    "$$\n",
    "(X_i \\perp X_j | C),\n",
    "$$\n",
    "\n",
    "lo cual no siempre es válido. Aún así es un modelo bastante utilizado por su simplicidad, y poque aunque la suposición de independencia no siempre se satisface, da resultados razonablemente buenos en ciertos tipos de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas y numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos del Titanic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajar solo con categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exlainers y target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de clase para encoding de varias columnas\n",
    "class LabelEncoderMulti(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        self.encoders = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = dict()\n",
    "        for feature in self.features:\n",
    "            self.encoders[feature] = LabelEncoder().fit(X[:, feature])\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        Xt = X.copy()\n",
    "        for feature in self.features:\n",
    "            Xt[:, feature] = self.encoders[feature].transform(X[:, feature])\n",
    "        \n",
    "        return Xt\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score de los datos de prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde una perspectiva Bayesiana, podemos representar las VA objetivo $y$ y parámetros $w$, datos $X$ de un modelo lineal $y=Xw + \\epsilon$ como:\n",
    "\n",
    "![lr](figures/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como no nos interesa modelar los datos, sino los parámetros, estudiamos la distribución posterior $P(w | X, y)$, la cual se puede reescribir como:\n",
    "\n",
    "$$\n",
    "P(w | X, y) = \\frac{P(y, w | X)}{P(y | X)}.\n",
    "$$\n",
    "\n",
    "Por otra parte, usando la definición de probabilidad condicional y la factorización inducida, tenemos que:\n",
    "\n",
    "$$\n",
    "P(y, w | X) = P(y | X, w) P(w),\n",
    "$$\n",
    "\n",
    "de donde\n",
    "\n",
    "$$\n",
    "P(w | X, y) = \\frac{P(y, w | X)}{P(y | X)} = \\frac{P(y | X, w) P(w)}{P(y | X)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas distribuciones tienen nombres muy específicos:\n",
    "\n",
    "- $P(w | X, y)$ se conoce como la **distribución posterior**.\n",
    "- $P(y | X, w)$ se conoce como la **función de verosimilitud**.\n",
    "- $P(w)$ se conoce como la **distribución previa**.\n",
    "- $P(y | X)$ se conoce como la **distribución de evidencia**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. La distribución posterior la queremos calcular a través de las demás.\n",
    "\n",
    "2. La función de verosimilitud la podemos suponer razonablemente como:\n",
    "\n",
    "$$\n",
    "P(y | X, w) = \\mathcal{N}(y | X w, \\sigma^2 I) \\propto \\exp\\left\\{-\\frac{1}{2 \\sigma^2} ||y - X w||^2\\right\\},\n",
    "$$\n",
    "\n",
    "3. De la misma forma, para codificar el conocimiento previo sobre los parámetros podemos suponer:\n",
    "\n",
    "$$\n",
    "P(w) = \\mathcal{N}(w | 0, \\gamma^2 I),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de evidencia es especialmente compleja. Para ver porqué, respondamos las preguntas:\n",
    "\n",
    "- ¿Qué significa $P(y | X)$?\n",
    "- ¿Qué implicaría conocer $P(y | X)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esto, los métodos Bayesianos rondan a través de evitar el cálculo de esta distribución a través de distintas maneras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimador MAP\n",
    "\n",
    "En general nos interesa encontrar la **distribución posterior**, sin embargo, para este ejemplo, encontraremos sólo **la moda** de la misma.\n",
    "\n",
    "De modo que para encontrar el valor más probable de los parámetros según la evidencia de los datos y el objetivo, hacemos:\n",
    "\n",
    "$$\n",
    "\\arg \\max_{w} P(w | X, y) = \\arg \\max_{w} \\frac{P(y, w | X)}{P(y | X)} = \\arg \\max_{w} P(y, w | X) = \\arg \\max_{w} P(y | X, w) P(w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Propiedad de los logaritmos:** dado que el logaritmo es una función creciente,\n",
    "\n",
    "$$\n",
    "\\arg \\max_x f(x) = \\arg \\max_x \\log f(x) = \\arg \\min_x - \\log f(x)\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que el anterior problema lo podemos reescribir como:\n",
    "\n",
    "$$\n",
    "\\arg \\min_w -\\log P(y | X, w) - \\log P(w)\n",
    "$$\n",
    "\n",
    "Desarrollando los logaritmos <font color=green>en el pizarrón</font>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "-\\log P(y | X, w) - \\log P(w) & = \\dots \\\\\n",
    "                              & = \\dots \\\\\n",
    "                              & = \\dots \\\\\n",
    "                              & = \\log C + \\frac{1}{2\\sigma^2} (y - X w)^T (y - X w) + \\frac{1}{2\\gamma^2} w^T w \\\\\n",
    "                              & = \\log C + \\frac{1}{2\\sigma^2} ||y - X w||^2 + \\frac{1}{2\\gamma^2} ||w||^2\n",
    "\\end{align}\n",
    "\n",
    "Finalmente, podemos resolver el problema \n",
    "\n",
    "$$\n",
    "\\arg \\min_w ||y - X w||^2 + \\lambda ||w||^2,\n",
    "$$\n",
    "\n",
    "con $\\lambda = \\frac{\\sigma^2}{\\gamma^2}$. <font color=green> Interpretación de $\\lambda$</font>.\n",
    "\n",
    "Se puede demostrar que la solución es:\n",
    "\n",
    "$$\n",
    "w^{*}_{reg} = (X^T X + \\lambda I)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar numpy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijar semilla para reproducibilidad\n",
    "\n",
    "# Dispersión de los datos\n",
    "\n",
    "# Generar datos (recta + ruido)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar sklearn.linear_model.Ridge\n",
    "\n",
    "# Importar sklearn.pipeline.Pipeline\n",
    "\n",
    "# Importar sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler-regressor pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros más probables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribución previa conjugada\n",
    "\n",
    "Bajo el framework de estimación MAP, quitamos el denominador puesto que este no dependía de $w$, que era sobre lo que estábamos maximizando.\n",
    "\n",
    "Esto lo podemos generalizar, dándonos cuenta que la distribución posterior $P(w | X, y)$ es distribución sobre $w$, y que la distribución de evidencia $P(y | X)$ es tan solo un factor **normalizante** sobre el producto $P(y | X, w) P(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿De qué nos sirve esto? Veamos:\n",
    "\n",
    "La función de verosimilitud $P(y | X, w)$ la suponemos una distribución normal de la forma:\n",
    "\n",
    "$$\n",
    "P(y | X, w) = \\mathcal{N}(y | X w, \\sigma^2 I) \\propto \\exp\\left\\{-\\frac{1}{2 \\sigma^2} ||y - X w||^2\\right\\}.\n",
    "$$\n",
    "\n",
    "Notemos que, para cualquier $w_0 \\in \\mathbb{R}^d$:\n",
    "\n",
    "\\begin{align}\n",
    "||y - X w||^2 & = ||X w - y||^2 \\\\\n",
    "              & = ||X (w - w_0) + X w_0 - y||^2 \\\\\n",
    "              & = \\left(X (w - w_0) + X w_0 - y\\right)^T \\left(X (w - w_0) + X w_0 - y\\right) \\\\\n",
    "              & = (w - w_0)^T X^T X (w - w_0) + 2 (w - w_0)^T X^T (X w_0 - y) + ||X w_0- y||^2 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular, haciendo $w_0 = w^* = \\left(X^T X\\right)^{-1} X^T y$ (solución de mínimos cuadrados), se observa que:\n",
    "\n",
    "\\begin{array}\n",
    "2 (w - w^*)^T X^T (X w^* - y) & = 2 (Xw - Xw^*)^T  (X w^* - y) \\\\\n",
    "                              & = 2 \\left[w^T X^T X w^* - w^T X^Ty - (w^*)^T X^T X w^* + (w^*)^T X^T y \\right]\\\\\n",
    "                              & = 2 \\left[w^T \\underbrace{X^T X \\left(X^T X\\right)^{-1}}_{I} X^T y - w^T X^Ty - y^T X \\underbrace{\\left(X^T X\\right)^{-1} X^T X}_{I} \\left(X^T X\\right)^{-1} X^T y + y^T X \\left(X^T X\\right)^{-1} X^T y\\right] \\\\\n",
    "                              & = 2 \\left[w^T X^T y - w^T X^Ty - y^T X  \\left(X^T X\\right)^{-1} X^T y + y^T X \\left(X^T X\\right)^{-1} X^T y\\right] \\\\\n",
    "                              & = 0.\n",
    "\\end{array}\n",
    "\n",
    "De esta manera: \n",
    "\n",
    "$$\n",
    "||y - X w||^2 = (w - w^*)^T X^T X (w - w^*) + \\underbrace{||X w^*- y||^2 }_{f(X, y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, concluimos que\n",
    "\n",
    "\\begin{align}\n",
    "P(y | X, w) & = \\mathcal{N}(y | X w, \\sigma^2 I) \\\\\n",
    "            & \\propto \\exp\\left\\{-\\frac{1}{2 \\sigma^2} ||y - X w||^2\\right\\} \\\\\n",
    "            & \\propto \\exp\\left\\{-\\frac{1}{2 \\sigma^2} (w - w^*)^T X^T X (w - w^*)\\right\\} \\\\\n",
    "            & \\propto \\mathcal{N}\\left(w | w^*, \\sigma^2 \\left(X^T X\\right)^{-1}\\right)\n",
    "\\end{align}\n",
    "\n",
    "#### ¡La función de verosimilitud es proporcional a una distribución Normal sobre los parámetros!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retomando la distribución posterior, sabemos que\n",
    "\n",
    "$$\n",
    "P(w | X, y) \\propto P(y | X, w) P(w) \\propto \\mathcal{N}\\left(w | w^*, \\sigma^2 \\left(X^T X\\right)^{-1}\\right) P(w).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "El producto de dos densidades normales univariadas es otra densidad normal univariada. Esto se extiende al caso multivariable con:\n",
    "\n",
    "$$\n",
    "\\mu_3 = \\Sigma_2 \\left(\\Sigma_1 + \\Sigma_2\\right)^{-1} \\mu_1 + \\Sigma_1 \\left(\\Sigma_1 + \\Sigma_2\\right)^{-1} \\mu_2, \\qquad \\Sigma_3 = \\Sigma_1 \\left(\\Sigma_1 + \\Sigma_2\\right)^{-1} \\Sigma_2,\n",
    "$$\n",
    "\n",
    "o equivalentemente\n",
    "\n",
    "$$\n",
    "\\Sigma_3^{-1} = \\Sigma_1^{-1} + \\Sigma_2^{-1} , \\qquad\n",
    "\\mu_3 = \\Sigma_2 \\Sigma_3^{-1} \\mu_1 + \\Sigma_1 \\Sigma_3^{-1}\\mu_2\n",
    "$$\n",
    "\n",
    "De manera que una previa conjugada para el caso anterior es:\n",
    "\n",
    "$$\n",
    "P(w) = \\mathcal{N}(w | 0, \\gamma^2 I),\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con lo cual:\n",
    "\n",
    "$$\n",
    "P(w | X, y) = \\mathcal{N}\\left(w | w^{**}, \\Sigma^{*}\\right).\n",
    "$$\n",
    "\n",
    "con\n",
    "\n",
    "$$\n",
    "\\left(\\Sigma^{*}\\right)^{-1} = \\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\gamma^2}I, \\qquad w^{**} = \\left(\\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\gamma^2}I \\right)^{-1}\\frac{1}{\\sigma^2} X^T X w^{*} = \\left(X^T X + \\frac{\\sigma^2}{\\gamma^2}I \\right)^{-1} X^T y = w^{*}_{reg}, \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya teníamos los datos. Graficamos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma\n",
    "\n",
    "# Distribución posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros distribución posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "w0 = np.linspace(3.8, 5.5, 100)\n",
    "w1 = np.linspace(1.9, 2.2, 100)\n",
    "w0, w1 = np.meshgrid(w0, w1)\n",
    "z = posterior.pdf(np.dstack([w0, w1]))\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(w0, w1, z)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(w0, w1, z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros más probables y diferentes posibilidades\n",
    "for i in range(100):\n",
    "    wp = posterior.rvs()\n",
    "    plt.plot(x, X.dot(wp), 'k', alpha=0.3)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, X.dot(posterior.mean), 'r', lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué pasa si el valor que usamos para $\\sigma$ se desvía del valor real?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma y sigma\n",
    "gamma = 10\n",
    "sigma = 10\n",
    "# Distribución posterior\n",
    "l = sigma**2 / gamma**2\n",
    "w_reg = np.linalg.inv(X.T.dot(X) + l * np.eye(2)).dot(X.T.dot(y))\n",
    "Sigma = np.linalg.inv(X.T.dot(X) / sigma**2 + np.eye(2) / gamma**2)\n",
    "posterior = stats.multivariate_normal(mean=w_reg, cov=Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "w0 = np.linspace(0, 9, 100)\n",
    "w1 = np.linspace(1, 3, 100)\n",
    "w0, w1 = np.meshgrid(w0, w1)\n",
    "z = posterior.pdf(np.dstack([w0, w1]))\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(w0, w1, z)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(w0, w1, z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros más probables y diferentes posibilidades\n",
    "for i in range(100):\n",
    "    wp = posterior.rvs()\n",
    "    plt.plot(x, X.dot(wp), 'k', alpha=0.3)\n",
    "plt.plot(x, X.dot(wp), 'k', alpha=0.3, label=\"Parámetros probables\")\n",
    "plt.plot(x, y, 'o', label=\"Datos\")\n",
    "plt.plot(x, X.dot(posterior.mean), 'r', lw=3, label=\"Parámetros más probables\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje en línea\n",
    "\n",
    "Este esquema que acabamos de ver es susceptible para aprendizaje en línea. Dado que la distribución posterior es normal cuando la previa es normal (previa conjugada), podemos usar esta distribución posterior para el siguiente paso. La nueva previa para el siguiente paso sería:\n",
    "\n",
    "$$\n",
    "P_{k+1}(w) = P_{k}(w | X, y) = \\mathcal{N}\\left(w | \\mu_k, \\Sigma_k\\right).\n",
    "$$\n",
    "\n",
    "**Tarea.** Dado que la función de verosimilitud satisface:\n",
    "\n",
    "$$\n",
    "P(y | X, w) \\propto \\mathcal{N}\\left(w | w^*, \\sigma^2 \\left(X^T X\\right)^{-1}\\right),\n",
    "$$\n",
    "\n",
    "¿Cuál sería la distribución posterior con una previa normal $P(w)  = \\mathcal{N}\\left(w | \\mu, \\Sigma\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
